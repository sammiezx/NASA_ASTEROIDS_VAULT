{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import time\n",
    "import logging\n",
    "\n",
    "from client import hit_nasa_api\n",
    "from adapter import parse_response\n",
    "from connector.cassandra_connector import get_session, create_and_set_keyspace, create_table, save_dataframe_to_cassandra, get_min_max_dates\n",
    "# from log.logger_set import get_logger\n",
    "\n",
    "class CatchStream:\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        with open(os.getenv(\"CONF_PATH\"), 'r') as file:\n",
    "            conf = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        \n",
    "        session = get_session()\n",
    "        create_and_set_keyspace(session, conf['cassandra_keyspace_name'])\n",
    "        create_table(session, conf['cassandra_table_name'])\n",
    "\n",
    "        stream_id = datetime.now().date().strftime('CATCH_STREAM_%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "        self.conf = conf\n",
    "        self.session = session\n",
    "        self.date_offset, _ = get_min_max_dates(session)\n",
    "        # self.logger = get_logger(conf['log_path']+ stream_id + '.log')\n",
    "\n",
    "        logging.basicConfig(filename= conf['log_path']+ stream_id + '.log', level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        self.logger = logging.getLogger(__name__ + f'.instance_{id(self)}')\n",
    "        \n",
    "\n",
    "    def clear_buffer(self):\n",
    "        file_path = self.conf['catchup_buffer_path']\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"File '{file_path}' has been deleted.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{file_path}' not found.\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "    def get_payload(self):\n",
    "        self.logger.info(\"[TARGETING NASA API]\")\n",
    "        conf = self.conf\n",
    "\n",
    "        date_offset = self.date_offset\n",
    "        if(date_offset < datetime.strptime(conf['catch_upto'], \"%Y-%m-%d\")):\n",
    "            raise Exception(\"CAUGHT UP!!, [you can disable the dag now]\")\n",
    "        \n",
    "        end_date = date_offset.strftime('%Y-%m-%d')\n",
    "        start_date = (date_offset - timedelta(days=6)).strftime('%Y-%m-%d')\n",
    "        self.date_offset = date_offset - timedelta(days=6)\n",
    "        print(start_date, end_date)\n",
    "\n",
    "        try:\n",
    "            df = hit_nasa_api(start_date, end_date)\n",
    "            df.to_csv(conf['catchup_buffer_path'], index=False)\n",
    "            self.logger.info(f\"[PAYLOAD INITIALIZED FOR]: {start_date} TO {end_date} batch\")\n",
    "            print(f\"[PAYLOAD INITIALIZED FOR]: {start_date} TO {end_date} batch\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def preprocess_payload(self):\n",
    "        self.logger.info(\"[PROCESSING PAYLOAD]\")\n",
    "        conf = self.conf\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(conf['catchup_buffer_path'])\n",
    "            df = parse_response(df)\n",
    "            # adapter takes care of most of the parsing and all, while more can be added in this task later on\n",
    "            self.clear_buffer()\n",
    "            df.to_csv(conf['catchup_buffer_path'], index=False)\n",
    "            self.logger.info(\"[PAYLOAD PROCESSED FOR]: {start_date} TO {end_date} batch\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def dump_payload(self):\n",
    "        self.logger.info(\"[DUMPING PAYLOAD]\")\n",
    "        conf = self.conf\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(conf['catchup_buffer_path'])\n",
    "            df['neo_reference_id'] = df['neo_reference_id'].astype(str)\n",
    "            df['miss_distance_astronomical'] = df['miss_distance_astronomical'].astype(str)\n",
    "            df['miss_distance_lunar'] = df['miss_distance_lunar'].astype(str)\n",
    "            df['id'] = df['id'].astype(str)\n",
    "\n",
    "            save_dataframe_to_cassandra(self.session, df, conf['cassandra_table_name'])\n",
    "            self.clear_buffer()\n",
    "            # self.logger.info(f\"[PAYLOAD DUMPED FOR]: {start_date} TO {end_date} batch\")\n",
    "            # print(f\"[PAYLOAD DUMPED FOR]: {start_date} TO {end_date} batch\")\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "\n",
    "    def stream(self):\n",
    "        while True:\n",
    "            try:\n",
    "                self.get_payload()\n",
    "                self.preprocess_payload()\n",
    "                self.dump_payload()\n",
    "                # time.sleep(100)\n",
    "            except Exception as e:\n",
    "                self.logger.error(\"An error occurred: %s\", str(e), exc_info=True)\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = CatchStream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-29 2023-08-04\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-07-29 TO 2023-08-04 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-07-23 2023-07-29\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-07-23 TO 2023-07-29 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-07-17 2023-07-23\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-07-17 TO 2023-07-23 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-07-11 2023-07-17\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-07-11 TO 2023-07-17 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-07-05 2023-07-11\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-07-05 TO 2023-07-11 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-06-29 2023-07-05\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-06-29 TO 2023-07-05 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-06-23 2023-06-29\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-06-23 TO 2023-06-29 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-06-17 2023-06-23\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-06-17 TO 2023-06-23 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-06-11 2023-06-17\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-06-11 TO 2023-06-17 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-06-05 2023-06-11\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-06-05 TO 2023-06-11 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-05-30 2023-06-05\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-05-30 TO 2023-06-05 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-05-24 2023-05-30\n",
      "[PAYLOAD INITIALIZED FOR]: 2023-05-24 TO 2023-05-30 batch\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "File '/home/samir/Desktop/ARIMA/nasa_asteroid_vault/warehouse/buffer/catchup_transient_df.csv' has been deleted.\n",
      "2023-05-18 2023-05-24\n"
     ]
    }
   ],
   "source": [
    "stream.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
